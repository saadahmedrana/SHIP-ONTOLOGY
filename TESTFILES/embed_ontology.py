# TESTFILES/embed_ontology.py
# One-time embedding of the ontology using Aalto text-embedding-3-large
# Only AALTO_KEY comes from environment

import os, json, numpy as np, time, requests
from dotenv import load_dotenv

# Load the .env file (from project root)
load_dotenv()

AALTO_KEY = os.getenv("AALTO_KEY")
if not AALTO_KEY:
    raise EnvironmentError("‚ö†Ô∏è  Please set AALTO_KEY in your .env file.")

# ---------- FILE PATHS ----------
SRC = "../withcomments.jsonld"  # ontology source file
CHUNKS_OUT = "chunks_full.jsonl"
VEC_OUT = "ontology_vectors.npy"
IDS_OUT = "ontology_ids.json"
TXT_OUT = "ontology_texts.json"

# ---------- API CONFIG ----------
EMBED_MODEL = "text-embedding-3-large"
EMBED_URL = "https://aalto-openai-apigw.azure-api.net/v1/openai/text-embedding-3-large/embeddings"
HEADERS = {
    "Content-Type": "application/json",
    "Ocp-Apim-Subscription-Key": AALTO_KEY
}

# ---------- SETTINGS ----------
BATCH_SIZE = 50
SLEEP_BETWEEN_CALLS = 1.0  # seconds


def flatten_value(v):
    if isinstance(v, dict):
        return "; ".join(f"{kk}: {flatten_value(vv)}" for kk, vv in v.items())
    if isinstance(v, list):
        return ", ".join(flatten_value(x) for x in v)
    return str(v)


def make_chunks(src_path, out_path):
    """Convert ontology JSON-LD into flattened text chunks."""
    with open(src_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    graph = data.get("@graph", [])
    count = 0
    with open(out_path, "w", encoding="utf-8") as w:
        for node in graph:
            if "@id" not in node:
                continue
            iri = node["@id"]
            lines = [f"Entity ID: {iri}."]
            for k, v in node.items():
                if k == "@id":
                    continue
                val = flatten_value(v).strip()
                if val:
                    lines.append(f"{k}: {val}.")
            text = " ".join(lines)
            chunk = {"id": iri, "text": text}
            w.write(json.dumps(chunk, ensure_ascii=False) + "\n")
            count += 1
    print(f"‚úÖ Created {count} chunks ‚Üí {out_path}")


def embed_batch(texts):
    payload = {"input": texts, "model": EMBED_MODEL}
    r = requests.post(EMBED_URL, headers=HEADERS, json=payload)
    if r.status_code != 200:
        raise RuntimeError(f"Embedding request failed: {r.status_code}\n{r.text[:500]}")
    return [item["embedding"] for item in r.json()["data"]]


def embed_all():
    with open(CHUNKS_OUT, "r", encoding="utf-8") as f:
        chunks = [json.loads(line) for line in f]

    texts = [c["text"] for c in chunks]
    ids = [c["id"] for c in chunks]

    embeddings = []
    total = len(texts)
    print(f"üß† Total chunks to embed: {total}")

    for i in range(0, total, BATCH_SIZE):
        batch = texts[i : i + BATCH_SIZE]
        print(f" ‚Üí Embedding batch {i//BATCH_SIZE+1} ({len(batch)} items)")
        embs = embed_batch(batch)
        embeddings.extend(embs)
        time.sleep(SLEEP_BETWEEN_CALLS)

    np.save(VEC_OUT, np.array(embeddings, dtype=np.float32))
    json.dump(ids, open(IDS_OUT, "w", encoding="utf-8"), indent=2)
    json.dump(texts, open(TXT_OUT, "w", encoding="utf-8"), indent=2)
    print(f"‚úÖ Embeddings saved: {len(embeddings)} ‚Üí {VEC_OUT}")


if __name__ == "__main__":
    # 1. Make chunks from ontology
    make_chunks(SRC, CHUNKS_OUT)

    # 2. Create embeddings (one-time)
    embed_all()
